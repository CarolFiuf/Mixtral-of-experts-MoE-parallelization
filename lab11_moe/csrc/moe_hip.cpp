// !!! This is a file automatically generated by hipify!!!
#include <pybind11/pybind11.h>
#include <torch/extension.h>
#include <hip/hip_runtime.h>
#include <cmath>
#include <limits>
#include <iostream> // For debugging
#include<ATen/hip/impl/HIPStreamMasqueradingAsCUDA.h>

// --- Configuration Constants ---
#define THREADS_PER_BLOCK_ORIG 256
#define TILE_SIZE_ORIG 128

// Kernel 1 (W1+SiLU) Config - For Medium Inter Dim Strategy
#define K1_THREADS_PER_BLOCK 256
#define K1_TILE_I 128
#define K1_TILE_J 64

// Kernel 2 (GEMM2) Config - For Medium Inter Dim Strategy
#define K2_THREADS_PER_BLOCK 256
#define K2_TILE_D 128
#define K2_TILE_K 32
#define K2_OUTPUTS_PER_THREAD 4
#define K2_K_BLOCK 8

// Configuration for Expert-centric approach (Large Inter Dim Strategy)
#define EXPERT_BLOCK_SIZE 256      // Thread-per-block cho expert kernels

// GEMM Tile dimensions handled by one WORKGROUP (block)
#define EXPERT_WG_TILE_M 64
#define EXPERT_WG_TILE_N 64
#define EXPERT_WG_TILE_K 32 // K tile size for shared memory loading

// Each THREAD computes a small block of the output tile
#define EXPERT_THREAD_TILE_M 4
#define EXPERT_THREAD_TILE_N 4

// Sanity check: Block size must match tile dimensions / thread tile dimensions
static_assert(EXPERT_BLOCK_SIZE == (EXPERT_WG_TILE_M / EXPERT_THREAD_TILE_M) * (EXPERT_WG_TILE_N / EXPERT_THREAD_TILE_N),
              "Expert kernel block size / tile dimensions mismatch");

// --- Original DeepSeek Kernel (Single Kernel Strategy - Small Inter Dim) ---
__global__ void moe_tiled_kernel(
    const float* __restrict__ hidden,
    const float* __restrict__ w1,
    const float* __restrict__ w2,
    const int32_t* __restrict__ topk_ids,
    const float* __restrict__ topk_w,
    float* __restrict__ out,
    int N_branches,
    int token_num,
    int topk,
    int model_dim,
    int inter_dim
) {
    // Một block cho mỗi branch (token/topk pair)
    int idx = blockIdx.x;
    if (idx >= N_branches) return;

    int eid = topk_ids[idx];
    int tok = idx / topk;
    float wgt = topk_w[idx];

    const float* h_ptr = hidden + (size_t)idx * model_dim;
    // W1 layout: [E, 2*inter_dim, model_dim]
    const float* w1_base = w1 + (size_t)eid * (2 * inter_dim) * model_dim;
    // W2 layout: [E, model_dim, inter_dim]
    const float* w2_base = w2 + (size_t)eid * model_dim * inter_dim;
    float* out_row = out + (size_t)tok * model_dim;

    // Shared memory for intermediate results (inter_dim elements)
    extern __shared__ float smem[];
    float* buf0 = smem; // Holds activated values (after SiLU * Up)
    float* buf1 = smem + TILE_SIZE_ORIG; // Temp buffer for Up projection

    // Process inter_dim in chunks matching shared memory size
    int n_chunks = (inter_dim + TILE_SIZE_ORIG - 1) / TILE_SIZE_ORIG;

    for (int c = 0; c < n_chunks; ++c) {
        int base_inter = c * TILE_SIZE_ORIG;
        int chunk_inter_size = min(TILE_SIZE_ORIG, inter_dim - base_inter);

        // Step 1 & 2: Compute Gate, Up Projections, Apply SiLU and Gating
        // Each thread computes one element of the intermediate (activated) dimension
        for (int i = threadIdx.x; i < chunk_inter_size; i += blockDim.x) {
            int inter_global_idx = base_inter + i; // Global index in inter_dim

            // Pointers to rows in W1 corresponding to this intermediate dimension index
            // W1 layout: [2*inter_dim, model_dim] (for this expert)
            const float* row_g = w1_base + (size_t)inter_global_idx * model_dim;
            const float* row_u = w1_base + (size_t)(inter_dim + inter_global_idx) * model_dim;

            double sum_g = 0.0;
            double sum_u = 0.0;

            // Dot product: hidden_vector * w1_row
            #pragma unroll 4
            for (int j = 0; j < model_dim; ++j) {
                double h = (double)h_ptr[j];
                sum_g = std::fma(h, (double)row_g[j], sum_g);
                sum_u = std::fma(h, (double)row_u[j], sum_u);
            }

            // Apply SiLU and Gating: activated = silu(gate) * up
            const double sig = 1.0 / (1.0 + std::exp(-sum_g));
            const double silu_g = sum_g * sig;
            const double activated_val = silu_g * sum_u;
            buf0[i] = (float)activated_val; // Store the final activated value for this inter_dim element
        }
        __syncthreads(); // Ensure all intermediate values in buf0 are computed

        // Step 3: Compute Output Projection (GEMV: activated_vector * W2)
        // Each thread computes one element of the output dimension (model_dim)
        // W2 layout: [model_dim, inter_dim] (for this expert)
        for (int d = threadIdx.x; d < model_dim; d += blockDim.x) {
            // Pointer to the start of the row in W2 corresponding to output dimension 'd'
            const float* w2_row = w2_base + (size_t)d * inter_dim;
            // Pointer to the part of the row relevant to the current inter_dim chunk
            const float* w2_row_chunk = w2_row + base_inter;

            double acc = 0.0;
            // Dot product: activated_chunk (buf0) * w2_row_chunk
            #pragma unroll 4
            for (int k = 0; k < chunk_inter_size; ++k) {
                 acc = std::fma((double)buf0[k], (double)w2_row_chunk[k], acc);
            }

            // Atomically add the contribution to the output tensor, scaled by the topk weight
            if (acc != 0.0) { // Avoid atomicAdd for zero contribution
                atomicAdd(&out_row[d], (float)acc * wgt);
            }
        }
        __syncthreads(); // Ensure atomic adds are complete before next chunk
    }
}

// --- Kernel 1: GEMM1 + SiLU (Medium Inter Dim Strategy) ---
__global__ void moe_intermediate_w1_kernel_optimized(
    const float* __restrict__ hidden,
    const float* __restrict__ w1,
    const int32_t* __restrict__ topk_ids,
    float* __restrict__ intermediate, // Output: [N_branches, inter_dim]
    int N_branches,
    int model_dim,
    int inter_dim
) {
    // One block per branch (token/topk pair)
    int idx = blockIdx.x;
    if (idx >= N_branches) return;

    int eid = topk_ids[idx];
    const float* h_ptr_global = hidden + (size_t)idx * model_dim; // Input hidden state for this branch
    // W1 layout: [E, 2*inter_dim, model_dim]
    const float* w1_base = w1 + (size_t)eid * (2 * inter_dim) * model_dim; // Expert's W1
    float* intermediate_row = intermediate + (size_t)idx * inter_dim; // Output row for this branch

    // Shared memory for a tile of hidden states (improves cache reuse for model_dim)
    extern __shared__ float h_smem[]; // Size: K1_TILE_J (e.g., 64)

    // Process inter_dim (output dimension) in tiles of size K1_TILE_I
    for (int i_base = 0; i_base < inter_dim; i_base += K1_TILE_I) {
        int i_chunk = min(K1_TILE_I, inter_dim - i_base);

        // Each thread calculates outputs for different inter_dim elements within the current i_chunk
        for (int i_offset = threadIdx.x; i_offset < i_chunk; i_offset += blockDim.x) {
            int i_global = i_base + i_offset; // Global inter_dim index

            // Pointers to W1 rows for the current inter_dim element
            const float* row_g_global = w1_base + (size_t)i_global * model_dim;
            const float* row_u_global = w1_base + (size_t)(inter_dim + i_global) * model_dim;

            double sum_g = 0.0;
            double sum_u = 0.0;

            // Process model_dim (input dimension) in tiles for better cache hits
            for (int j_base = 0; j_base < model_dim; j_base += K1_TILE_J) {
                int j_chunk = min(K1_TILE_J, model_dim - j_base);

                // Load hidden state tile into shared memory cooperatively
                // Each thread loads one element (or handle boundary if j_chunk < blockDim.x)
                int load_offset = threadIdx.x;
                 if (load_offset < j_chunk) {
                    h_smem[load_offset] = h_ptr_global[j_base + load_offset];
                }
                __syncthreads(); // Ensure hidden state tile is loaded

                // Compute partial dot products using the shared memory tile
                const float* row_g_chunk = row_g_global + j_base;
                const float* row_u_chunk = row_u_global + j_base;

                // Use doubles for accumulation within the chunk
                double partial_sum_g = 0.0;
                double partial_sum_u = 0.0;
                for (int j_offset = 0; j_offset < j_chunk; ++j_offset) {
                    double h_val = (double)h_smem[j_offset]; // Read from shared memory
                    partial_sum_g = std::fma(h_val, (double)row_g_chunk[j_offset], partial_sum_g);
                    partial_sum_u = std::fma(h_val, (double)row_u_chunk[j_offset], partial_sum_u);
                }
                // Accumulate partial sums
                sum_g += partial_sum_g;
                sum_u += partial_sum_u;

                __syncthreads(); // Optional: Wait before next j_tile iteration (may help scheduling)
            } // End loop over model_dim (j) tiles

            // Apply SiLU and Gating
            const double sig = 1.0 / (1.0 + std::exp(-sum_g));
            const double silu_g = sum_g * sig;
            const double gated_val = silu_g * sum_u;

            // Write result to the intermediate tensor
            intermediate_row[i_global] = (float)gated_val;
        } // End loop over inter_dim (i) elements within the chunk
         // Implicit __syncthreads() before next i_base iteration is handled by kernel execution model,
         // but good practice to ensure all writes are visible if needed later within the block (not needed here).
    } // End loop over inter_dim (i) tiles
}


// --- Kernel 2: GEMM2 (Medium Inter Dim Strategy) ---
__global__ void moe_gemm2_kernel(
    const float* __restrict__ intermediate, // Input: [N_branches, inter_dim]
    const float* __restrict__ w2,           // Weights: [E, model_dim, inter_dim]
    const int32_t* __restrict__ topk_ids,
    const float* __restrict__ topk_w,
    float* __restrict__ out,                // Output: [token_num, model_dim]
    int N_branches,
    int token_num,
    int topk,
    int model_dim,  // Output dimension D
    int inter_dim   // Reduction dimension K
) {
    // Grid: (N_branches, (model_dim + K2_TILE_D - 1) / K2_TILE_D)
    int branch_idx = blockIdx.x;
    int d_tile_idx = blockIdx.y; // Index of the output dimension tile

    if (branch_idx >= N_branches) return;

    int d_start = d_tile_idx * K2_TILE_D; // Start index in model_dim for this block
    if (d_start >= model_dim) return;
    int d_end = min(d_start + K2_TILE_D, model_dim); // End index
    int d_tile_size = d_end - d_start; // Actual size of this output tile (<= K2_TILE_D)

    int eid = topk_ids[branch_idx];
    int tok = branch_idx / topk; // Original token index
    float wgt = topk_w[branch_idx]; // Weight for this branch

    // Input vector for this branch
    const float* intermediate_row = intermediate + (size_t)branch_idx * inter_dim;
    // Expert's W2 matrix [model_dim, inter_dim] -> [D, K]
    const float* w2_expert = w2 + (size_t)eid * model_dim * inter_dim;
    // Output row corresponding to the original token
    float* out_row = out + (size_t)tok * model_dim;

    // Shared memory: K2_TILE_K for intermediate tile + K2_TILE_D * K2_TILE_K for W2 tile
    extern __shared__ float smem[];
    float* inter_tile = smem;                  // Size: K2_TILE_K
    float* w2_tile = smem + K2_TILE_K;         // Size: K2_TILE_D * K2_TILE_K (Layout: row-major D * K)

    // Accumulators in registers (per thread) for K2_OUTPUTS_PER_THREAD output elements
    double thread_results[K2_OUTPUTS_PER_THREAD] = {0.0};

    // Process inter_dim (reduction dimension K) in chunks of K2_TILE_K
    int k_chunks = (inter_dim + K2_TILE_K - 1) / K2_TILE_K;

    for (int kc = 0; kc < k_chunks; ++kc) {
        int k_start = kc * K2_TILE_K;
        int k_tile_size = min(K2_TILE_K, inter_dim - k_start); // Actual size of this K chunk

        // --- Load intermediate tile (vector chunk) into shared memory ---
        // Cooperatively load k_tile_size elements
        for (int k_load_idx = threadIdx.x; k_load_idx < k_tile_size; k_load_idx += blockDim.x) {
            inter_tile[k_load_idx] = intermediate_row[k_start + k_load_idx];
        }

        // --- Load W2 tile into shared memory ---
        // Cooperatively load d_tile_size * k_tile_size elements
        // W2 layout: [D, K]. We need the tile corresponding to rows d_start..d_end-1 and cols k_start..k_end-1
        // Load into w2_tile[d_local, k_local] -> stored at d_local * K2_TILE_K + k_local
        int w2_tile_elements_to_load = d_tile_size * k_tile_size;
        for (int load_idx = threadIdx.x; load_idx < w2_tile_elements_to_load; load_idx += blockDim.x) {
            int d_local = load_idx / k_tile_size; // Row within the D tile (0..d_tile_size-1)
            int k_local = load_idx % k_tile_size; // Col within the K tile (0..k_tile_size-1)

            int d_global = d_start + d_local;     // Global D index (model_dim)
            int k_global = k_start + k_local;     // Global K index (inter_dim)

            // Access w2_expert[d_global, k_global]
            w2_tile[d_local * K2_TILE_K + k_local] = w2_expert[(size_t)d_global * inter_dim + k_global];
        }
        __syncthreads(); // Ensure both tiles are loaded

        // --- Compute matrix multiplication contribution ---
        // Each thread computes K2_OUTPUTS_PER_THREAD elements of the output tile D
        #pragma unroll
        for (int out_idx = 0; out_idx < K2_OUTPUTS_PER_THREAD; ++out_idx) {
            // Calculate the local output dimension index (d_local) this thread instance handles
            int d_local = threadIdx.x + out_idx * blockDim.x;

            if (d_local < d_tile_size) { // Check if within bounds of the current D tile
                // Use K2_K_BLOCK for register blocking on the K dimension
                for (int k = 0; k < k_tile_size; k += K2_K_BLOCK) {
                    // Preload intermediate values (A) and weights (B) into registers
                    float k_vals[K2_K_BLOCK];
                    float w_vals[K2_K_BLOCK];

                    #pragma unroll
                    for (int kb = 0; kb < K2_K_BLOCK; ++kb) {
                        if (k + kb < k_tile_size) {
                            k_vals[kb] = inter_tile[k + kb]; // Load from shared memory A tile
                            w_vals[kb] = w2_tile[d_local * K2_TILE_K + k + kb]; // Load from shared memory B tile
                        } else {
                            k_vals[kb] = 0.0f;
                            w_vals[kb] = 0.0f;
                        }
                    }

                    // Perform FMA operations C += A * B
                    #pragma unroll
                    for (int kb = 0; kb < K2_K_BLOCK; ++kb) {
                         // Check not needed again due to zero padding above, but safe
                         if (k + kb < k_tile_size) {
                            thread_results[out_idx] = std::fma((double)k_vals[kb], (double)w_vals[kb], thread_results[out_idx]);
                         }
                    }
                } // End loop over K blocks
            } // End check d_local bounds
        } // End loop over outputs per thread
        __syncthreads(); // Ensure computation is done before loading next k chunk
    } // End loop over K chunks

    // --- Write accumulated results to global memory ---
    // Atomically add the weighted result to the final output tensor
    #pragma unroll
    for (int out_idx = 0; out_idx < K2_OUTPUTS_PER_THREAD; ++out_idx) {
        int d_local = threadIdx.x + out_idx * blockDim.x;
        if (d_local < d_tile_size) {
            int d_global = d_start + d_local;
            if (thread_results[out_idx] != 0.0) { // Avoid atomicAdd for zero
                atomicAdd(&out_row[d_global], (float)thread_results[out_idx] * wgt);
            }
        }
    }
}


// --- Expert-centric approach kernels (Large Inter Dim Strategy) ---

// Helper function để tạo masks và indices cho từng expert
__global__ void create_expert_masks(
    const int32_t* __restrict__ topk_ids,    // Input: [token_num, topk]
    const float* __restrict__ topk_w,        // Input: [token_num, topk]
    int32_t* __restrict__ expert_indices,    // Output: [E, token_num * topk] (stores original token_idx)
    float* __restrict__ expert_weights,      // Output: [E, token_num * topk] (stores corresponding weight)
    int32_t* __restrict__ expert_counts,     // Output: [E] (stores count per expert)
    int token_num,
    int topk,
    int num_experts
) {
    // Each block handles one expert
    int expert_id = blockIdx.x;
    if (expert_id >= num_experts) return;

    // Reset counter for the current expert (only thread 0 does this)
    if (threadIdx.x == 0) {
        expert_counts[expert_id] = 0;
    }
    __syncthreads(); // Ensure counter is reset before threads proceed

    // Each thread checks a subset of the flattened token/topk pairs
    // Total pairs = token_num * topk
    for (int flat_idx = threadIdx.x; flat_idx < token_num * topk; flat_idx += blockDim.x) {
        int token_idx = flat_idx / topk; // Original token index
        int k_idx = flat_idx % topk;    // Index within the top-k choices for the token

        // Check if this token/topk pair belongs to the current expert
        int current_expert_id = topk_ids[token_idx * topk + k_idx];
        if (current_expert_id == expert_id) {
            // Atomically increment the counter and get the offset for this expert
            int offset = atomicAdd(&expert_counts[expert_id], 1);

            // Store the original token index and its weight at the calculated offset
            // Note: Max offset is token_num * topk - 1. Ensure buffers are large enough.
            // Storing sequentially per expert. Index into buffers: expert_id * max_tokens_per_expert + offset
            // Assuming max_tokens_per_expert >= token_num * topk is potentially wasteful,
            // use a large enough stride like token_num * topk.
            size_t base_offset = (size_t)expert_id * token_num * topk;
            expert_indices[base_offset + offset] = token_idx;
            expert_weights[base_offset + offset] = topk_w[token_idx * topk + k_idx];
        }
    }
}

// Kernel để tập hợp các tokens thuộc một expert
__global__ void gather_expert_tokens(
    const float* __restrict__ hidden,        // Input: [token_num, model_dim] (Original hidden states)
    float* __restrict__ expert_tokens,       // Output: [token_count, model_dim] (Contiguous buffer for this expert)
    const int32_t* __restrict__ expert_indices,// Input: [E, token_num * topk] (Indices generated by create_expert_masks)
    int expert_id,                           // Current expert ID
    int token_count,                         // Number of tokens assigned to this expert
    int token_num,                           // Total number of tokens
    int topk,                                // Top-k value
    int model_dim                            // Hidden dimension size
) {
    // Skip if this expert has no tokens
    if (token_count == 0) return;

    // Each thread copies one element of the expert's token data
    // Total elements to copy = token_count * model_dim
    size_t total_elements = (size_t)token_count * model_dim;
    size_t base_expert_indices_offset = (size_t)expert_id * token_num * topk;

    for (size_t idx = (size_t)blockIdx.x * blockDim.x + threadIdx.x;
         idx < total_elements;
         idx += (size_t)gridDim.x * blockDim.x)
    {
        int expert_token_offset = idx / model_dim; // Which token *within this expert's batch* (0 to token_count-1)
        int feature_idx = idx % model_dim;       // Which feature dimension

        // Get the original token index from the expert_indices buffer
        int original_token_idx = expert_indices[base_expert_indices_offset + expert_token_offset];

        // Read from the original hidden states and write to the contiguous expert buffer
        expert_tokens[idx] = hidden[(size_t)original_token_idx * model_dim + feature_idx];
    }
}


// Refactored GEMM kernel for W1 (Gate and Up) using standard tiling
__global__ void expert_gemm_w1_kernel(
    const float* __restrict__ expert_tokens, // Input A: [token_count, model_dim]
    const float* __restrict__ w1,            // Input B (Weights): [E, 2*inter_dim, model_dim]
    float* __restrict__ gate_out,            // Output C1: [token_count, inter_dim]
    float* __restrict__ up_out,              // Output C2: [token_count, inter_dim]
    int expert_id,
    int token_count, // M dimension
    int model_dim,   // K dimension
    int inter_dim    // N dimension
) {
    if (token_count == 0) return;

    // Block dimensions define the size of the C tile computed by the block (WG = WorkGroup)
    int wg_tile_m = blockIdx.x * EXPERT_WG_TILE_M;
    int wg_tile_n = blockIdx.y * EXPERT_WG_TILE_N;

    // Pointers to weight matrices for the current expert
    // W1 layout: [E, 2*inter_dim, model_dim] -> Treat as [E][2N][K]
    const float* w1_expert_base = w1 + (size_t)expert_id * (2 * inter_dim) * model_dim;
    const float* w1_gate_expert = w1_expert_base;                    // Points to [inter_dim, model_dim] part -> [N, K]
    const float* w1_up_expert   = w1_expert_base + (size_t)inter_dim * model_dim; // Points to second [inter_dim, model_dim] part -> [N, K]

    // Thread identification within the block
    int thread_id = threadIdx.x;
    const int threads_per_row = EXPERT_WG_TILE_N / EXPERT_THREAD_TILE_N; // e.g., 64/4 = 16
    int thread_tile_m_base = (thread_id / threads_per_row) * EXPERT_THREAD_TILE_M; // Thread row start in C tile (0, 4, 8, ...)
    int thread_tile_n_base = (thread_id % threads_per_row) * EXPERT_THREAD_TILE_N; // Thread col start in C tile (0, 4, 8, ...)

    // Shared memory allocation
    extern __shared__ float smem[];
    // Tile for A (expert_tokens): WG_M x WG_K
    float* token_tile = smem;
    // Tile for B (w1_gate weights): WG_K x WG_N
    float* w1_gate_tile = token_tile + EXPERT_WG_TILE_M * EXPERT_WG_TILE_K;
    // Tile for B (w1_up weights): WG_K x WG_N
    float* w1_up_tile = w1_gate_tile + EXPERT_WG_TILE_K * EXPERT_WG_TILE_N;


    // Accumulators in registers for the C tile computed by this thread
    float gate_accum[EXPERT_THREAD_TILE_M][EXPERT_THREAD_TILE_N] = {{0.0f}};
    float up_accum[EXPERT_THREAD_TILE_M][EXPERT_THREAD_TILE_N] = {{0.0f}};

    // Loop over K dimension tiles (model_dim)
    for (int k_tile_base = 0; k_tile_base < model_dim; k_tile_base += EXPERT_WG_TILE_K) {
        int k_tile_size = min(EXPERT_WG_TILE_K, model_dim - k_tile_base);

        // --- Load A tile (expert_tokens) into Shared Memory ---
        // Cooperatively load WG_M * k_tile_size elements
        int num_a_elements = EXPERT_WG_TILE_M * k_tile_size;
        for (int load_idx = thread_id; load_idx < num_a_elements; load_idx += blockDim.x) {
            int m_local = load_idx / k_tile_size; // Row within A tile (0..WG_M-1)
            int k_local = load_idx % k_tile_size; // Col within A tile (0..k_tile_size-1)
            int m_global = wg_tile_m + m_local;
            int k_global = k_tile_base + k_local;

            if (m_global < token_count && k_global < model_dim) { // Boundary check M and K
                 token_tile[m_local * EXPERT_WG_TILE_K + k_local] = expert_tokens[(size_t)m_global * model_dim + k_global];
            } else {
                 token_tile[m_local * EXPERT_WG_TILE_K + k_local] = 0.0f; // Padding
            }
        }

        // --- Load B tile (w1_gate) into Shared Memory ---
        // Cooperatively load k_tile_size * WG_N elements
        // W1 Gate: [N, K] layout in memory
        int num_b_gate_elements = k_tile_size * EXPERT_WG_TILE_N;
         for (int load_idx = thread_id; load_idx < num_b_gate_elements; load_idx += blockDim.x) {
            int k_local = load_idx / EXPERT_WG_TILE_N; // Row within B tile (corresponds to K)
            int n_local = load_idx % EXPERT_WG_TILE_N; // Col within B tile (corresponds to N)
            int k_global = k_tile_base + k_local;
            int n_global = wg_tile_n + n_local;

            if (n_global < inter_dim && k_global < model_dim) { // Boundary check N and K
                // Accessing w1_gate_expert[n_global, k_global]
                w1_gate_tile[k_local * EXPERT_WG_TILE_N + n_local] = w1_gate_expert[(size_t)n_global * model_dim + k_global];
            } else {
                w1_gate_tile[k_local * EXPERT_WG_TILE_N + n_local] = 0.0f; // Padding
            }
        }

        // --- Load B tile (w1_up) into Shared Memory ---
        // W1 Up: [N, K] layout in memory
        int num_b_up_elements = k_tile_size * EXPERT_WG_TILE_N;
         for (int load_idx = thread_id; load_idx < num_b_up_elements; load_idx += blockDim.x) {
            int k_local = load_idx / EXPERT_WG_TILE_N;
            int n_local = load_idx % EXPERT_WG_TILE_N;
            int k_global = k_tile_base + k_local;
            int n_global = wg_tile_n + n_local;

            if (n_global < inter_dim && k_global < model_dim) { // Boundary check N and K
                 // Accessing w1_up_expert[n_global, k_global]
                w1_up_tile[k_local * EXPERT_WG_TILE_N + n_local] = w1_up_expert[(size_t)n_global * model_dim + k_global];
            } else {
                w1_up_tile[k_local * EXPERT_WG_TILE_N + n_local] = 0.0f; // Padding
            }
        }

        __syncthreads(); // Ensure tiles are loaded before computation

        // --- Compute C tile contribution ---
        // C[m, n] += A[m, k] * B[k, n]
        // A = token_tile, B = w1_gate_tile / w1_up_tile (indexed as B[k, n])
        #pragma unroll
        for (int k_local = 0; k_local < k_tile_size; ++k_local) {
            #pragma unroll
            for (int m_offset = 0; m_offset < EXPERT_THREAD_TILE_M; ++m_offset) {
                // Fetch A[m, k] into register (m = thread_tile_m_base + m_offset)
                float a_val = token_tile[(thread_tile_m_base + m_offset) * EXPERT_WG_TILE_K + k_local];

                #pragma unroll
                for (int n_offset = 0; n_offset < EXPERT_THREAD_TILE_N; ++n_offset) {
                    // Fetch B[k, n] into registers (n = thread_tile_n_base + n_offset)
                    float b_gate_val = w1_gate_tile[k_local * EXPERT_WG_TILE_N + (thread_tile_n_base + n_offset)];
                    float b_up_val   = w1_up_tile[k_local * EXPERT_WG_TILE_N + (thread_tile_n_base + n_offset)];

                    // FMA: C[m, n] += A[m, k] * B[k, n]
                    gate_accum[m_offset][n_offset] = std::fma(a_val, b_gate_val, gate_accum[m_offset][n_offset]);
                    up_accum[m_offset][n_offset]   = std::fma(a_val, b_up_val, up_accum[m_offset][n_offset]);
                }
            }
        }
        __syncthreads(); // Ensure computation with this k-tile is done before loading next
    } // End k_tile loop

    // --- Write results from registers to Global Memory ---
    #pragma unroll
    for (int m_offset = 0; m_offset < EXPERT_THREAD_TILE_M; ++m_offset) {
        int m_local = thread_tile_m_base + m_offset;
        int m_global = wg_tile_m + m_local;

        // Boundary check M
        if (m_global < token_count) {
            #pragma unroll
            for (int n_offset = 0; n_offset < EXPERT_THREAD_TILE_N; ++n_offset) {
                int n_local = thread_tile_n_base + n_offset;
                int n_global = wg_tile_n + n_local;

                // Boundary check N (inter_dim)
                if (n_global < inter_dim) {
                    // Write gate result: C1[m, n]
                    gate_out[(size_t)m_global * inter_dim + n_global] = gate_accum[m_offset][n_offset];
                    // Write up result: C2[m, n]
                    up_out[(size_t)m_global * inter_dim + n_global]   = up_accum[m_offset][n_offset];
                }
            }
        }
    }
}


// Kernel để áp dụng SiLU và gating (element-wise)
__global__ void expert_silu_gate_kernel(
    const float* __restrict__ gate_in,     // Input: [token_count, inter_dim]
    const float* __restrict__ up_in,       // Input: [token_count, inter_dim]
    float* __restrict__ activated_out,     // Output: [token_count, inter_dim]
    int token_count,
    int inter_dim
) {
    if (token_count == 0) return;
    // Each thread processes one element
    size_t total_elements = (size_t)token_count * inter_dim;
    for (size_t idx = (size_t)blockIdx.x * blockDim.x + threadIdx.x;
         idx < total_elements;
         idx += (size_t)gridDim.x * blockDim.x)
    {
        float gate_val = gate_in[idx];
        float up_val = up_in[idx];

        // SiLU activation: x * sigmoid(x)
        float sig = 1.0f / (1.0f + expf(-gate_val)); // Use expf for float
        float silu = gate_val * sig;

        // Gating
        activated_out[idx] = silu * up_val;
    }
}

// Refactored GEMM kernel for W2 using standard tiling
__global__ void expert_gemm_w2_kernel(
    const float* __restrict__ activated,     // Input A: [token_count, inter_dim]
    const float* __restrict__ w2,            // Input B (Weights): [E, model_dim, inter_dim]
    float* __restrict__ expert_output,       // Output C: [token_count, model_dim]
    int expert_id,
    int token_count, // M dimension
    int model_dim,   // N dimension
    int inter_dim    // K dimension
) {
     if (token_count == 0) return;

    // Block dimensions define the size of the C tile computed by the block (WG = WorkGroup)
    int wg_tile_m = blockIdx.x * EXPERT_WG_TILE_M;
    int wg_tile_n = blockIdx.y * EXPERT_WG_TILE_N; // N is model_dim here

    // Pointer to weight matrix for the current expert
    // W2 layout: [E, model_dim, inter_dim] -> Treat as [E][N][K]
    const float* w2_expert = w2 + (size_t)expert_id * model_dim * inter_dim;

    // Thread identification within the block
    int thread_id = threadIdx.x;
    const int threads_per_row = EXPERT_WG_TILE_N / EXPERT_THREAD_TILE_N; // e.g., 64/4 = 16
    int thread_tile_m_base = (thread_id / threads_per_row) * EXPERT_THREAD_TILE_M;
    int thread_tile_n_base = (thread_id % threads_per_row) * EXPERT_THREAD_TILE_N;

    // Shared memory allocation
    extern __shared__ float smem[];
    // Tile for A (activated): WG_M x WG_K
    float* activated_tile = smem;
    // Tile for B (w2 weights): WG_K x WG_N
    float* w2_tile = activated_tile + EXPERT_WG_TILE_M * EXPERT_WG_TILE_K;

    // Accumulators in registers for the C tile computed by this thread
    float output_accum[EXPERT_THREAD_TILE_M][EXPERT_THREAD_TILE_N] = {{0.0f}};

    // Loop over K dimension tiles (inter_dim)
    for (int k_tile_base = 0; k_tile_base < inter_dim; k_tile_base += EXPERT_WG_TILE_K) {
        int k_tile_size = min(EXPERT_WG_TILE_K, inter_dim - k_tile_base);

        // --- Load A tile (activated) into Shared Memory ---
        // Cooperatively load WG_M * k_tile_size elements
        int num_a_elements = EXPERT_WG_TILE_M * k_tile_size;
        for (int load_idx = thread_id; load_idx < num_a_elements; load_idx += blockDim.x) {
            int m_local = load_idx / k_tile_size;
            int k_local = load_idx % k_tile_size;
            int m_global = wg_tile_m + m_local;
            int k_global = k_tile_base + k_local;

            if (m_global < token_count && k_global < inter_dim) { // Boundary check M and K
                 activated_tile[m_local * EXPERT_WG_TILE_K + k_local] = activated[(size_t)m_global * inter_dim + k_global];
            } else {
                 activated_tile[m_local * EXPERT_WG_TILE_K + k_local] = 0.0f; // Padding
            }
        }

        // --- Load B tile (w2) into Shared Memory ---
        // Cooperatively load k_tile_size * WG_N elements
        // W2: [N, K] layout in memory
        int num_b_elements = k_tile_size * EXPERT_WG_TILE_N;
         for (int load_idx = thread_id; load_idx < num_b_elements; load_idx += blockDim.x) {
            int k_local = load_idx / EXPERT_WG_TILE_N; // Row within B tile (K)
            int n_local = load_idx % EXPERT_WG_TILE_N; // Col within B tile (N)
            int k_global = k_tile_base + k_local;
            int n_global = wg_tile_n + n_local;

            if (n_global < model_dim && k_global < inter_dim) { // Boundary check N and K
                // Accessing w2_expert[n_global, k_global]
                w2_tile[k_local * EXPERT_WG_TILE_N + n_local] = w2_expert[(size_t)n_global * inter_dim + k_global];
            } else {
                w2_tile[k_local * EXPERT_WG_TILE_N + n_local] = 0.0f; // Padding
            }
        }

        __syncthreads(); // Ensure tiles are loaded before computation

        // --- Compute C tile contribution ---
        // C[m, n] += A[m, k] * B[k, n]
        // A = activated_tile, B = w2_tile (indexed as B[k, n])
        #pragma unroll
        for (int k_local = 0; k_local < k_tile_size; ++k_local) {
            #pragma unroll
            for (int m_offset = 0; m_offset < EXPERT_THREAD_TILE_M; ++m_offset) {
                // Fetch A[m, k] into register
                float a_val = activated_tile[(thread_tile_m_base + m_offset) * EXPERT_WG_TILE_K + k_local];

                #pragma unroll
                for (int n_offset = 0; n_offset < EXPERT_THREAD_TILE_N; ++n_offset) {
                    // Fetch B[k, n] into register
                    float b_val = w2_tile[k_local * EXPERT_WG_TILE_N + (thread_tile_n_base + n_offset)];

                    // FMA: C[m, n] += A[m, k] * B[k, n]
                    output_accum[m_offset][n_offset] = std::fma(a_val, b_val, output_accum[m_offset][n_offset]);
                }
            }
        }
        __syncthreads(); // Ensure computation is done before loading next k-tile
    } // End k_tile loop

    // --- Write results from registers to Global Memory ---
    #pragma unroll
    for (int m_offset = 0; m_offset < EXPERT_THREAD_TILE_M; ++m_offset) {
        int m_local = thread_tile_m_base + m_offset;
        int m_global = wg_tile_m + m_local;

        // Boundary check M
        if (m_global < token_count) {
            #pragma unroll
            for (int n_offset = 0; n_offset < EXPERT_THREAD_TILE_N; ++n_offset) {
                int n_local = thread_tile_n_base + n_offset;
                int n_global = wg_tile_n + n_local;

                // Boundary check N (model_dim)
                if (n_global < model_dim) {
                    // Write output result: C[m, n]
                    expert_output[(size_t)m_global * model_dim + n_global] = output_accum[m_offset][n_offset];
                }
            }
        }
    }
}


// Kernel để cập nhật kết quả cuối cùng với weights (scatter using atomicAdd)
__global__ void scatter_expert_outputs(
    float* __restrict__ out,                 // Output: [token_num, model_dim] (Final output tensor)
    const float* __restrict__ expert_output, // Input: [token_count, model_dim] (Output of expert_gemm_w2)
    const int32_t* __restrict__ expert_indices,// Input: [E, token_num * topk] (Original token indices)
    const float* __restrict__ expert_weights,  // Input: [E, token_num * topk] (Corresponding weights)
    int expert_id,
    int token_count,
    int token_num,
    int topk,
    int model_dim
) {
    if (token_count == 0) return;

    // Each thread scatters one element of the expert's output
    size_t total_elements = (size_t)token_count * model_dim;
    size_t base_expert_info_offset = (size_t)expert_id * token_num * topk;

    for (size_t idx = (size_t)blockIdx.x * blockDim.x + threadIdx.x;
         idx < total_elements;
         idx += (size_t)gridDim.x * blockDim.x)
    {
        int expert_token_offset = idx / model_dim; // Which token *within this expert's batch*
        int feature_idx = idx % model_dim;       // Which feature dimension

        // Get the original token index and the corresponding weight
        int original_token_idx = expert_indices[base_expert_info_offset + expert_token_offset];
        float weight = expert_weights[base_expert_info_offset + expert_token_offset];

        // Get the computed result for this element from the expert's output buffer
        float result = expert_output[idx]; // expert_output is already [token_count, model_dim]

        // Atomically add the weighted result to the final output tensor
        if (result != 0.0f && weight != 0.0f) { // Avoid atomicAdd if contribution is zero
            atomicAdd(&out[(size_t)original_token_idx * model_dim + feature_idx], result * weight);
        }
    }
}

// Launcher function
torch::Tensor launch_custom_moe(
    torch::Tensor hidden_states,
    torch::Tensor w1,
    torch::Tensor w2,
    torch::Tensor topk_weight,
    torch::Tensor topk_ids
) {
    // Input validation
    TORCH_CHECK(hidden_states.is_cuda(), "hidden_states must be a CUDA tensor");
    TORCH_CHECK(w1.is_cuda(), "w1 must be a CUDA tensor");
    TORCH_CHECK(w2.is_cuda(), "w2 must be a CUDA tensor");
    TORCH_CHECK(topk_weight.is_cuda(), "topk_weight must be a CUDA tensor");
    TORCH_CHECK(topk_ids.is_cuda(), "topk_ids must be a CUDA tensor");

    // Type checks
    TORCH_CHECK(hidden_states.scalar_type() == torch::kFloat32, "hidden_states must be float32");
    TORCH_CHECK(w1.scalar_type() == torch::kFloat32, "w1 must be float32");
    TORCH_CHECK(w2.scalar_type() == torch::kFloat32, "w2 must be float32");
    TORCH_CHECK(topk_weight.scalar_type() == torch::kFloat32, "topk_weight must be float32");
    TORCH_CHECK(topk_ids.scalar_type() == torch::kInt32, "topk_ids must be int32");

    // Dimension checks
    TORCH_CHECK(hidden_states.dim() == 2, "hidden_states must be 2D"); // [token_num, model_dim]
    TORCH_CHECK(w1.dim() == 3, "w1 must be 3D"); // [E, 2*inter_dim, model_dim]
    TORCH_CHECK(w2.dim() == 3, "w2 must be 3D"); // [E, model_dim, inter_dim]
    TORCH_CHECK(topk_weight.dim() == 2, "topk_weight must be 2D"); // [token_num, topk]
    TORCH_CHECK(topk_ids.dim() == 2, "topk_ids must be 2D"); // [token_num, topk]

    // Extract dimensions
    const int token_num = hidden_states.size(0);
    const int model_dim = hidden_states.size(1);
    const int topk = topk_weight.size(1);
    const int E = w1.size(0);  // Số experts
    const int inter_dim = w2.size(2);
    const int N_branches = token_num * topk; // Total number of token-expert assignments

    // Dimension consistency checks
    TORCH_CHECK(w1.size(1) == 2 * inter_dim, "w1 dimension 1 mismatch (expected 2*inter_dim)");
    TORCH_CHECK(w1.size(2) == model_dim, "w1 dimension 2 mismatch (expected model_dim)");
    TORCH_CHECK(w2.size(0) == E, "w2 dimension 0 mismatch (expected E)");
    TORCH_CHECK(w2.size(1) == model_dim, "w2 dimension 1 mismatch (expected model_dim)");
    TORCH_CHECK(topk_weight.size(0) == token_num, "topk_weight dimension 0 mismatch (expected token_num)");
    TORCH_CHECK(topk_ids.size(0) == token_num, "topk_ids dimension 0 mismatch (expected token_num)");
    TORCH_CHECK(topk_ids.size(1) == topk, "topk_ids dimension 1 mismatch (expected topk)");

    // Initialize output tensor (zeros)
    auto out = torch::zeros_like(hidden_states);

    // Get default HIP stream
    hipStream_t stream = at::hip::getCurrentHIPStream(); // Use PyTorch's current stream

    // --- STRATEGY SELECTION based on inter_dim ---

    // Case 1: Small inter_dim (e.g., <= 512) - Use Single Kernel (Original DeepSeek style)
    // This strategy is often efficient when inter_dim is small enough that the intermediate
    // activations fit reasonably well in shared memory or caches.
    if (inter_dim <= 512 && model_dim >= 1024) { // Heuristic threshold
        // Prepare flattened inputs needed by the kernel
        // Reshape hidden_states: [T, D] -> [T, 1, D] -> [T, K, D] -> [T*K, D]
        auto hidden_flat = hidden_states.unsqueeze(1)
                                      .expand({-1, topk, -1}) // Use expand for efficiency
                                      .reshape({N_branches, model_dim})
                                      .contiguous(); // Ensure contiguity
        // Reshape weights and IDs: [T, K] -> [T*K]
        auto topk_w_flat = topk_weight.reshape({N_branches}).contiguous();
        auto topk_ids_flat = topk_ids.reshape({N_branches}).contiguous();

        // Launch single kernel
        dim3 grid(N_branches); // One block per token-expert branch
        dim3 block(THREADS_PER_BLOCK_ORIG); // e.g., 256
        // Shared memory size calculation: Needs 2 * TILE_SIZE_ORIG floats
        size_t smem_size = sizeof(float) * 2 * TILE_SIZE_ORIG;

        hipLaunchKernelGGL(
            moe_tiled_kernel,
            grid, block, smem_size, stream,
            hidden_flat.data_ptr<float>(),
            w1.data_ptr<float>(),
            w2.data_ptr<float>(),
            topk_ids_flat.data_ptr<int32_t>(),
            topk_w_flat.data_ptr<float>(),
            out.data_ptr<float>(),
            N_branches, token_num, topk, model_dim, inter_dim
        );
    }
    // Case 2: Medium inter_dim (e.g., > 512 and <= 2048) - Use Two Kernel Strategy
    // Splits into (GEMM1+SiLU) and (GEMM2). Reduces shared memory pressure compared to single kernel.
    else if (inter_dim <= 2048) { // Heuristic threshold
        // Prepare flattened inputs (same as Case 1)
        auto hidden_flat = hidden_states.unsqueeze(1)
                                      .expand({-1, topk, -1})
                                      .reshape({N_branches, model_dim})
                                      .contiguous();
        auto topk_w_flat = topk_weight.reshape({N_branches}).contiguous();
        auto topk_ids_flat = topk_ids.reshape({N_branches}).contiguous();

        // Allocate intermediate tensor: [N_branches, inter_dim]
        auto intermediate = torch::empty({N_branches, inter_dim}, hidden_states.options());

        // --- Launch Kernel 1: W1 + SiLU ---
        dim3 grid1(N_branches); // One block per branch
        dim3 block1(K1_THREADS_PER_BLOCK); // e.g., 256
        // Shared memory for hidden state tile
        size_t smem1_size = sizeof(float) * K1_TILE_J; // e.g., 64 floats

        hipLaunchKernelGGL(
            moe_intermediate_w1_kernel_optimized,
            grid1, block1, smem1_size, stream,
            hidden_flat.data_ptr<float>(),
            w1.data_ptr<float>(),
            topk_ids_flat.data_ptr<int32_t>(),
            intermediate.data_ptr<float>(),
            N_branches, model_dim, inter_dim
        );

        // --- Launch Kernel 2: GEMM2 ---
        // Grid dimensions: (N_branches, Num D tiles)
        int num_d_tiles = (model_dim + K2_TILE_D - 1) / K2_TILE_D;
        dim3 grid2(N_branches, num_d_tiles);
        dim3 block2(K2_THREADS_PER_BLOCK); // e.g., 256
        // Shared memory: Intermediate tile + W2 tile
        size_t smem2_size = sizeof(float) * (K2_TILE_K + K2_TILE_D * K2_TILE_K);

        hipLaunchKernelGGL(
            moe_gemm2_kernel,
            grid2, block2, smem2_size, stream,
            intermediate.data_ptr<float>(),
            w2.data_ptr<float>(),
            topk_ids_flat.data_ptr<int32_t>(),
            topk_w_flat.data_ptr<float>(),
            out.data_ptr<float>(),
            N_branches, token_num, topk, model_dim, inter_dim
        );
    }
    // Case 3: Large inter_dim (e.g., > 2048) - Use Expert-Centric Strategy
    // Reorganizes computation around experts, potentially better for very large inter_dim
    // where intermediate tensor [N_branches, inter_dim] becomes huge.
    else {
        // Expert-centric approach with separate CUDA kernels

        // Step 1: Allocate GPU buffers for expert routing information
        // Max possible items per expert is token_num * topk. Stride by this amount.
        size_t expert_info_stride_sz = (size_t)token_num * topk; // Keep as size_t for calculations
        int64_t expert_info_stride = static_cast<int64_t>(expert_info_stride_sz);
        auto expert_indices = torch::empty({E, expert_info_stride},
                                           torch::dtype(torch::kInt32).device(hidden_states.device()));
        auto expert_weights = torch::empty({E, expert_info_stride},
                                            torch::dtype(torch::kFloat32).device(hidden_states.device()));
        auto expert_counts = torch::zeros({E},
                                           torch::dtype(torch::kInt32).device(hidden_states.device()));

        // Step 2: Launch kernel to populate routing info (count tokens per expert and their indices/weights)
        {
            dim3 grid_mask(E); // One block per expert
            dim3 block_mask(EXPERT_BLOCK_SIZE); // Use configured block size

            hipLaunchKernelGGL(
                create_expert_masks,
                grid_mask, block_mask, 0, stream,
                topk_ids.data_ptr<int32_t>(),
                topk_weight.data_ptr<float>(),
                expert_indices.data_ptr<int32_t>(),
                expert_weights.data_ptr<float>(),
                expert_counts.data_ptr<int32_t>(),
                token_num, topk, E
            );
        }

        // Step 3: Copy expert counts from GPU to CPU to control the loop
        // (Alternatively, use GPU-side loop or persistent threads, but CPU control is simpler here)
        auto expert_counts_cpu = expert_counts.to(torch::kCPU, /*non_blocking=*/false); // Ensure copy completes
        auto expert_counts_data = expert_counts_cpu.accessor<int32_t, 1>();

        // Step 4: Loop through each expert and process its assigned tokens
        for (int expert_id = 0; expert_id < E; ++expert_id) {
            int token_count = expert_counts_data[expert_id]; // Number of tokens for this expert

            // Skip if expert has no tokens assigned
            if (token_count == 0) continue;

            // Step 4.1: Allocate temporary GPU buffers for this expert's data
            // Sizes are based on token_count for this expert
            auto expert_tokens = torch::empty({token_count, model_dim}, hidden_states.options());
            auto gate_out = torch::empty({token_count, inter_dim}, hidden_states.options());
            auto up_out = torch::empty({token_count, inter_dim}, hidden_states.options());
            auto activated = torch::empty({token_count, inter_dim}, hidden_states.options());
            auto expert_output = torch::empty({token_count, model_dim}, hidden_states.options());

            // Step 4.2: Gather tokens for the current expert
            {
                // Grid size can be tuned. Aim for good occupancy.
                // Max grid dim size is usually 65535 or higher.
                size_t gather_elements = (size_t)token_count * model_dim;
                int gather_grid_size = min((gather_elements + EXPERT_BLOCK_SIZE - 1) / EXPERT_BLOCK_SIZE, (size_t)65535);
                dim3 grid_gather(gather_grid_size);
                dim3 block_gather(EXPERT_BLOCK_SIZE);

                hipLaunchKernelGGL(
                    gather_expert_tokens,
                    grid_gather, block_gather, 0, stream,
                    hidden_states.data_ptr<float>(),
                    expert_tokens.data_ptr<float>(),
                    expert_indices.data_ptr<int32_t>(),
                    expert_id, token_count, token_num, topk, model_dim
                );
            }

            // Step 4.3: Compute W1 GEMM (Gate and Up projections)
            {
                int m_tiles = (token_count + EXPERT_WG_TILE_M - 1) / EXPERT_WG_TILE_M;
                int n_tiles = (inter_dim + EXPERT_WG_TILE_N - 1) / EXPERT_WG_TILE_N; // N = inter_dim

                dim3 grid_w1(m_tiles, n_tiles);
                dim3 block_w1(EXPERT_BLOCK_SIZE);
                size_t smem_w1 = sizeof(float) * (
                    EXPERT_WG_TILE_M * EXPERT_WG_TILE_K + // token_tile (A)
                    EXPERT_WG_TILE_K * EXPERT_WG_TILE_N + // w1_gate_tile (B1)
                    EXPERT_WG_TILE_K * EXPERT_WG_TILE_N   // w1_up_tile (B2)
                );

                hipLaunchKernelGGL(
                    expert_gemm_w1_kernel,
                    grid_w1, block_w1, smem_w1, stream,
                    expert_tokens.data_ptr<float>(),
                    w1.data_ptr<float>(),
                    gate_out.data_ptr<float>(),
                    up_out.data_ptr<float>(),
                    expert_id, token_count, model_dim, inter_dim
                );
            }

            // Step 4.4: Apply SiLU and Gating (element-wise)
            {
                size_t silu_elements = (size_t)token_count * inter_dim;
                int silu_grid_size = min((silu_elements + EXPERT_BLOCK_SIZE - 1) / EXPERT_BLOCK_SIZE, (size_t)65535);
                dim3 grid_silu(silu_grid_size);
                dim3 block_silu(EXPERT_BLOCK_SIZE);

                hipLaunchKernelGGL(
                    expert_silu_gate_kernel,
                    grid_silu, block_silu, 0, stream,
                    gate_out.data_ptr<float>(),
                    up_out.data_ptr<float>(),
                    activated.data_ptr<float>(),
                    token_count, inter_dim
                );
            }

            // Step 4.5: Compute W2 GEMM
            {
                int m_tiles = (token_count + EXPERT_WG_TILE_M - 1) / EXPERT_WG_TILE_M;
                int n_tiles = (model_dim + EXPERT_WG_TILE_N - 1) / EXPERT_WG_TILE_N; // N = model_dim

                dim3 grid_w2(m_tiles, n_tiles);
                dim3 block_w2(EXPERT_BLOCK_SIZE);
                size_t smem_w2 = sizeof(float) * (
                    EXPERT_WG_TILE_M * EXPERT_WG_TILE_K + // activated_tile (A)
                    EXPERT_WG_TILE_K * EXPERT_WG_TILE_N   // w2_tile (B)
                );

                hipLaunchKernelGGL(
                    expert_gemm_w2_kernel,
                    grid_w2, block_w2, smem_w2, stream,
                    activated.data_ptr<float>(),
                    w2.data_ptr<float>(),
                    expert_output.data_ptr<float>(),
                    expert_id, token_count, model_dim, inter_dim
                );
            }

            // Step 4.6: Scatter expert results back to final output tensor (atomicAdd)
            {
                size_t scatter_elements = (size_t)token_count * model_dim;
                int scatter_grid_size = min((scatter_elements + EXPERT_BLOCK_SIZE - 1) / EXPERT_BLOCK_SIZE, (size_t)65535);
                dim3 grid_scatter(scatter_grid_size);
                dim3 block_scatter(EXPERT_BLOCK_SIZE);

                hipLaunchKernelGGL(
                    scatter_expert_outputs,
                    grid_scatter, block_scatter, 0, stream,
                    out.data_ptr<float>(),
                    expert_output.data_ptr<float>(),
                    expert_indices.data_ptr<int32_t>(),
                    expert_weights.data_ptr<float>(),
                    expert_id, token_count, token_num, topk, model_dim
                );
            }
        } // End loop over experts
    }

    // Check for any HIP errors during kernel launches
    hipError_t err = hipGetLastError();
    TORCH_CHECK(err == hipSuccess, "HIP error after launching MoE kernels: ", hipGetErrorString(err));

    // Synchronize stream if necessary (though PyTorch usually handles this)
    // hipStreamSynchronize(stream);

    return out;
}

// PyTorch binding
PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("launch_custom_moe", &launch_custom_moe,
          "Launch optimized MoE kernel with strategy selection (Single, Two-Kernel, Expert-Centric)");
}